# Config for training tokenizer + building LM dataset (Tinystories 1M setup)

dataset:
  hf_dataset: roneneldan/TinyStories
  config: default
  split: train
  num_rows: 200000
  text_key: text

tokenizer:
  vocab_size: 10000
  min_frequency: 2
  model_type: bpe
  special_tokens:
    - "<pad>"
    - "<bos>"
    - "<eos>"
    - "<unk>"

  # how many examples to train tokenizer on (can equal num_rows)
  num_samples: 200000

  # sequence length for LM blocks
  block_size: 256

  # output locations used later by mamba_main.py
  save_dir: ./tokenizers/tinystories_1M
  lm_dataset_dir: ./lm_dataset/tinystories_1M

processing:
  lowercase: false
  strip_spaces: true

seed: 42
